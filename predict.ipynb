{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Learning\n",
    "\n",
    "ARC\n",
    "\n",
    "Tanmay Singh\n",
    "2021569\n",
    "CSAI\n",
    "Class of '25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/arc_data/ARC-AGI-master/data/'\n",
    "# training_dir = os.path.join(data_dir, 'training')\n",
    "evaluation_dir = os.path.join(data_dir, 'evaluation')\n",
    "\n",
    "val_split = 0.2\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# train_files = [os.path.join(training_dir, f) for f in os.listdir(training_dir) if f.endswith('.json')]\n",
    "evaluation_files = [os.path.join(evaluation_dir, f) for f in os.listdir(evaluation_dir) if f.endswith('.json')]\n",
    "\n",
    "# random.shuffle(train_files)\n",
    "# train_files, val_files = train_test_split(train_files, test_size=val_split, random_state=random_seed)\n",
    "\n",
    "def process_files(file_list, key='train'):\n",
    "    dataset = []\n",
    "    for file_path in file_list:\n",
    "        data = load_json(file_path)\n",
    "        for item in data[key]:\n",
    "            dataset.append({\n",
    "                'input': item['input'],\n",
    "                'output': item['output']\n",
    "            })\n",
    "    return dataset\n",
    "\n",
    "# train_dataset = process_files(train_files, key='train')\n",
    "# val_dataset = process_files(val_files, key='train')\n",
    "eval_dataset = process_files(evaluation_files, key='test')\n",
    "\n",
    "output_dir = './data/arc_data/processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'train_dataset.json'), 'w') as f:\n",
    "#     json.dump(train_dataset, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'val_dataset.json'), 'w') as f:\n",
    "#     json.dump(val_dataset, f, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'eval_dataset.json'), 'w') as f:\n",
    "    json.dump(eval_dataset, f, indent=4)\n",
    "\n",
    "# print(f\"Training samples: {len(train_dataset)}\")\n",
    "# print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_grid_uniform(grid, target_size=30, pad_value=10):\n",
    "    rows, cols = len(grid), len(grid[0]) if grid else 0\n",
    "\n",
    "    row_padding = (target_size - rows) // 2\n",
    "    col_padding = (target_size - cols) // 2\n",
    "\n",
    "    row_padding_extra = (target_size - rows) % 2\n",
    "    col_padding_extra = (target_size - cols) % 2\n",
    "\n",
    "    padded_grid = np.full((target_size, target_size), pad_value, dtype=int)\n",
    "\n",
    "    padded_grid[\n",
    "        row_padding : row_padding + rows, \n",
    "        col_padding : col_padding + cols\n",
    "    ] = grid\n",
    "\n",
    "    return padded_grid.tolist()\n",
    "\n",
    "def pad_dataset_uniform(dataset, target_size=30, pad_value=10):\n",
    "    padded_dataset = []\n",
    "    for task in dataset:\n",
    "        padded_task = {\n",
    "            \"input\": pad_grid_uniform(task[\"input\"], target_size, pad_value),\n",
    "            \"output\": pad_grid_uniform(task[\"output\"], target_size, pad_value)\n",
    "        }\n",
    "        padded_dataset.append(padded_task)\n",
    "    return padded_dataset\n",
    "\n",
    "output_dir = './data/arc_data/processed_data'\n",
    "\n",
    "# with open(os.path.join(output_dir, 'train_dataset.json'), 'r') as f:\n",
    "#     train_dataset = json.load(f)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'val_dataset.json'), 'r') as f:\n",
    "#     val_dataset = json.load(f)\n",
    "\n",
    "with open(os.path.join(output_dir, 'eval_dataset.json'), 'r') as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "# padded_train_dataset = pad_dataset_uniform(train_dataset, target_size=30, pad_value=10)\n",
    "# padded_val_dataset = pad_dataset_uniform(val_dataset, target_size=30, pad_value=10)\n",
    "padded_eval_dataset = pad_dataset_uniform(eval_dataset, target_size=30, pad_value=10)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'padded_train_dataset.json'), 'w') as f:\n",
    "#     json.dump(padded_train_dataset, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'padded_val_dataset.json'), 'w') as f:\n",
    "#     json.dump(padded_val_dataset, f, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'padded_eval_dataset.json'), 'w') as f:\n",
    "    json.dump(padded_eval_dataset, f, indent=4)\n",
    "\n",
    "print(\"Padding complete!\")\n",
    "# print(f\"Padded Training samples: {len(padded_train_dataset)}\")\n",
    "# print(f\"Padded Validation samples: {len(padded_val_dataset)}\")\n",
    "print(f\"Padded Evaluation samples: {len(padded_eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_grid(grid):\n",
    "    return [cell for row in grid for cell in row]\n",
    "\n",
    "def flatten_dataset(dataset):\n",
    "    flattened_dataset = []\n",
    "    for task in dataset:\n",
    "        flattened_task = {\n",
    "            \"input\": flatten_grid(task[\"input\"]),\n",
    "            \"output\": flatten_grid(task[\"output\"])\n",
    "        }\n",
    "        flattened_dataset.append(flattened_task)\n",
    "    return flattened_dataset\n",
    "\n",
    "# flattened_train_dataset = flatten_dataset(padded_train_dataset)\n",
    "# flattened_val_dataset = flatten_dataset(padded_val_dataset)\n",
    "flattened_eval_dataset = flatten_dataset(padded_eval_dataset)\n",
    "\n",
    "output_dir = './data/arc_data/processed_data'\n",
    "\n",
    "# with open(os.path.join(output_dir, 'flattened_train_dataset.json'), 'w') as f:\n",
    "#     json.dump(flattened_train_dataset, f, indent=4)\n",
    "\n",
    "# with open(os.path.join(output_dir, 'flattened_val_dataset.json'), 'w') as f:\n",
    "#     json.dump(flattened_val_dataset, f, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'flattened_eval_dataset.json'), 'w') as f:\n",
    "    json.dump(flattened_eval_dataset, f, indent=4)\n",
    "\n",
    "print(\"Flattened datasets saved!\")\n",
    "# print(f\"Flattened Training samples: {len(flattened_train_dataset)}\")\n",
    "# print(f\"Flattened Validation samples: {len(flattened_val_dataset)}\")\n",
    "print(f\"Flattened Evaluation samples: {len(flattened_eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 100\n",
    "INNER_LR = 1e-4\n",
    "OUTER_LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedARCDataset(Dataset):\n",
    "    def __init__(self, data, is_input=True):\n",
    "        self.data = data\n",
    "        self.is_input = is_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_input:\n",
    "            grid = self.data[idx][\"input\"]\n",
    "        else:\n",
    "            grid = self.data[idx][\"output\"]\n",
    "        return torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "eval_inputs = FlattenedARCDataset(flattened_eval_dataset, is_input=True)\n",
    "eval_outputs = FlattenedARCDataset(flattened_eval_dataset, is_input=False)\n",
    "\n",
    "eval_input_loader = DataLoader(eval_inputs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "eval_output_loader = DataLoader(eval_outputs, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strict_match_evaluation(model, input_loader, output_loader, device):\n",
    "    model.eval()\n",
    "    strict_matches = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(zip(input_loader, output_loader), desc=\"Strict Match Evaluation\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            predicted_outputs = torch.round(outputs).cpu().numpy()\n",
    "            target_outputs = targets.cpu().numpy()\n",
    "\n",
    "            for predicted, target in zip(predicted_outputs, target_outputs):\n",
    "                if np.array_equal(predicted, target):\n",
    "                    strict_matches += 1\n",
    "                total_samples += 1\n",
    "\n",
    "    accuracy = (strict_matches / total_samples) * 100\n",
    "    print(f\"Strict Match Accuracy: {accuracy:.2f}% ({strict_matches}/{total_samples})\")\n",
    "    return accuracy\n",
    "\n",
    "model_path = os.path.join(\"./saved_models\", \"best_meta_model.pth\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# model.load_state_dict(torch.load(\"best_meta_model.pth\"))\n",
    "model.to(DEVICE)\n",
    "\n",
    "strict_match_accuracy = strict_match_evaluation(model, eval_input_loader, eval_output_loader, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
