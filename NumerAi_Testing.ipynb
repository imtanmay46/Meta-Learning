{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Learning\n",
    "\n",
    "NumerAi\n",
    "\n",
    "Tanmay Singh\n",
    "2021569\n",
    "CSAI\n",
    "Class of '25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install xgboost\n",
    "!pip install seaborn\n",
    "!pip install pyarrow\n",
    "!pip install numerapi\n",
    "!pip install imblearn\n",
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install matplotlib\n",
    "!pip install cloudpickle\n",
    "!pip install mplcyberpunk\n",
    "!pip install scikit-learn\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import cloudpickle as cp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from numerapi import NumerAPI\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.cluster._kmeans import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumerAi = NumerAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = NumerAi.list_datasets()\n",
    "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "print(\"Available versions:\\n\", dataset_versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"v5.0\"\n",
    "\n",
    "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "print(\"availbable\", DATA_VERSION, \"files:\\n\", current_version_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumerAi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "for metadata in feature_metadata:\n",
    "  print(metadata, len(feature_metadata[metadata]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "for feature_set in [\"small\", \"medium\", \"all\"]:\n",
    "  print(feature_set, len(feature_sets[feature_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "feature_sets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_set in feature_sets:\n",
    "  print(f'Feature Set: {feature_set:<25}', f'Size: {len(feature_sets[feature_set])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Saved Experts & the Meta-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert1.pkl', 'rb') as f:\n",
    "    expert1 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert2.pkl', 'rb') as f:\n",
    "    expert2 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert3.pkl', 'rb') as f:\n",
    "    expert3 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert4.pkl', 'rb') as f:\n",
    "    expert4 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert5.pkl', 'rb') as f:\n",
    "    expert5 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_expert6.pkl', 'rb') as f:\n",
    "    expert6 = pickle.load(f)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_meta_model.pkl', 'rb') as f:\n",
    "    meta_model = pickle.load(f)\n",
    "print(\"Meta Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Validation Set, with a 'medium' feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = feature_sets[\"medium\"]\n",
    "\n",
    "NumerAi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
    "\n",
    "val = pd.read_parquet(\n",
    "    f\"{DATA_VERSION}/validation.parquet\",\n",
    "    columns=[\"era\", \"target\"] + feature_set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Validation Set (in the same manner as the Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.rename(columns=lambda x: f'feature {feature_set.index(x)}' if x in feature_set else x, inplace=True)\n",
    "feature_set = val.columns.drop([\"era\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val['era'] = val['era'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_era = val['era'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[val['era'] == unique_era[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = val\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.isna().any().any()\n",
    "test_set['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the Numeric Values in the Target into corresponding labels (class 0 to class 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(test_set['target'])\n",
    "test_set['target'] = label_encoder.transform(test_set['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_x = test_set.drop(['target'], axis=1, inplace=False)\n",
    "test_df_y = test_set['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************************************************************************************************************\n",
    "\"DUE TO RESOURCE CONSTRAINTS, THIS FILE WAS RUN WITH AT MAX 10 LAKH ENTRIES FOR 4 TIMES, ACCOUNTING TO THE SIZE OF THE VALIDATION SET\"\n",
    "**************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_x_resampled = test_df_x[:1000000]          #Run-1\n",
    "# test_df_x_resampled = test_df_x[1000000:2000000] #Run-2\n",
    "# test_df_x_resampled = test_df_x[2000000:3000000] #Run-3\n",
    "# test_df_x_resampled = test_df_x[3000000:]        #Run-4\n",
    "\n",
    "test_df_x_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_y_resampled = test_df_y[:1000000]          #Run-1\n",
    "# test_df_y_resampled = test_df_y[1000000:2000000] #Run-2\n",
    "# test_df_y_resampled = test_df_y[2000000:3000000] #Run-3\n",
    "# test_df_y_resampled = test_df_y[3000000:]        #Run-4\n",
    "\n",
    "test_df_y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute Label Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_frequency(predictions):\n",
    "    unique, counts = np.unique(predictions, return_counts=True)\n",
    "    label_frequencies = dict(zip(unique, counts))\n",
    "    print(\"Label frequencies:\", label_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute NumerAi Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerai_corr(preds, target):\n",
    "  ranked_preds = (preds.rank(method=\"average\").values - 0.5) / preds.count()\n",
    "  gauss_ranked_preds = stats.norm.ppf(ranked_preds)\n",
    "\n",
    "  centered_target = target - target.mean()\n",
    "\n",
    "  preds_p15 = np.sign(gauss_ranked_preds) * np.abs(gauss_ranked_preds) ** 1.5\n",
    "  target_p15 = np.sign(centered_target) * np.abs(centered_target) ** 1.5\n",
    "\n",
    "  return np.corrcoef(preds_p15, target_p15)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Predictions from Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-1 (XGBOOST CLASSIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert1_test_pred = expert1.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert1_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-2 (RANDOM FOREST CLASSIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert2_test_pred = expert2.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert2_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-3 (ADABOOST CLASSIFIER with DECISION TREE CLASSIFIER as BASE ESTIMATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert3_test_pred = expert3.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert3_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-4 (LOGISTIC REGRESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert4_test_pred = expert4.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert4_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-5 (CATBOOST CLASSIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert5_test_pred = expert5.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if expert5_test_pred.ndim > 1:\n",
    "    expert5_test_pred = expert5_test_pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert5_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERT-6 (HISTOGRAM-BASED GRADIENT BOOST CLASSIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert6_test_pred = expert6.predict(test_df_x_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert6_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Predictions from the Meta-Model (LIGHTGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_x = np.column_stack((expert1_test_pred, expert2_test_pred, expert3_test_pred, expert4_test_pred, expert5_test_pred, expert6_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_y_pred = meta_model.predict(meta_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.5, 1.5, 2.5, 3.5]\n",
    "\n",
    "rounded_predictions = np.digitize(meta_test_y_pred, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_frequency(rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Relevant Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(rounded_predictions, test_df_y_resampled)\n",
    "print(\"Accuracy on Validation Set: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson's Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr, _ = stats.pearsonr(rounded_predictions, test_df_y_resampled)\n",
    "print(\"Pearson Correlation:\", pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Predictions in a Pickle File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('numerai_prediction1.pkl', 'wb') as f:\n",
    "    pickle.dump(rounded_predictions, f)\n",
    "\n",
    "print(\"Predictions saved successfully to numerai_prediction1.pkl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('numerai_prediction2.pkl', 'wb') as f:\n",
    "#     pickle.dump(rounded_predictions, f)\n",
    "\n",
    "# print(\"Predictions saved successfully to numerai_prediction2.pkl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('numerai_prediction3.pkl', 'wb') as f:\n",
    "#     pickle.dump(rounded_predictions, f)\n",
    "\n",
    "# print(\"Predictions saved successfully to numerai_prediction3.pkl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('numerai_prediction4.pkl', 'wb') as f:\n",
    "#     pickle.dump(rounded_predictions, f)\n",
    "\n",
    "# print(\"Predictions saved successfully to numerai_prediction4.pkl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the NumerAi's Correlation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = pd.Series(rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_corr = numerai_corr(rounded_predictions, test_df_y_resampled)\n",
    "actual_corr"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
